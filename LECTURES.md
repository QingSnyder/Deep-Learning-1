# Course topics

## 1. Introduction to Python and Numpy
**Lectures:**
1. [How to use Google Colab for Python programming?](https://www.youtube.com/watch?v=PVsS9WtwVB8)
2. [Python3](https://youtube.com/watch?v=V42qfAPybp8)
3. [Numpy](https://www.youtube.com/watch?v=Omz8P8n-5gY)
4. [Matplotlib & Plotly](https://youtu.be/aIzkkjRzVdA) 

**Notebooks:** 
1. [Python3](notebooks/python.ipynb)
1. [Numpy](notebooks/numpy.ipynb)
1. [Matplotlib & Plotly](notebooks/matplotlib_plotly.ipynb) 

* If you are new to Python, please practice Python at [codewars.org](https://www.codewars.com/)  
* If you already have some Python and Numpy background, you may practice these:   
    1. [From Python to Numpy](https://www.labri.fr/perso/nrougier/from-python-to-numpy/)   
    2. [100 numpy exercises](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises.ipynb) by Nicolas P. Rougier  

## 2. Introduction to deep learning (Sections 1.1, 1.2, 1.3, and 4.1)
**Lectures:**
1. [Difference between AI, ML, and DL]
1. [Introduction to deep learning]
1. [The power of a hidden layer in neural networks]
1. [How does machine learning (or deep learning) work? The intuition]
1. [The four branches of machine learning]
1. [Learning 'bleeding-edge' deep learning]

## 3. Data representations & tensor operations (Sections 2.2, 2.3, and 2.4) 
**Lectures:**
1. [What are tensors? Matrix vs Tensor](https://youtu.be/7FeO4lqcNfA)
1. [Tensors reshape automatically](https://youtu.be/92gOeXFq2FA)
1. [Examples of 3D, 4D, and 5D tensors](https://youtu.be/8gOg4VNRUaY)
1. [The gears of neural networks: Tensor operations](https://youtu.be/rv9w4MfnWgQ)
1. [Geometric interpretation of deep learning](https://youtu.be/h30cyYjXFIU)

**Optional:** [Lecture on TF2 by Joshwa Gordon @ Google](https://youtu.be/5ECD8J3dvDQ)

## 4. Introduction to Keras (Sections 3.2 and 3.3) 
**Lectures:**
1. [Introduction to Keras](https://youtu.be/Ym34JC2UDFk)
1. [Keras is also an API in Tensorflow2](https://youtu.be/yNsQ6rqEcv4)
1. [Keras sequential vs functional API](https://youtu.be/EvGS3VAsG4Y)
1. [Diversity of thought is holding back AI & deep learning research](https://youtu.be/pXMFMs1ryy4)
1. [AlphaFold2: Example of the power of diversity](https://youtu.be/gg7WjuFs8F4)

**Optional:** [Francois Chollet interview](https://youtu.be/Bo8MY4JpiXE)

## 5. Training must stop!
**Lecture:** [Splitting data into development set + (training & validation) and test set + Callbacks](https://youtu.be/XajIX1X1cWQ)

## 6. Feed-forward neural networks
**Lecture:** [Binary classification using feed-forward neural networks](https://youtu.be/cJ3oqHqRBF0)    
**Notebook:** [Binary classification using feed-forward neural networks](./notebooks/wine_quality.ipynb)

## 7. Preparing images for deep learning (Sections 3.6.2, 5.2.4, and 5.2.5)
**Lectures:** 
1. [Image is all numbers](https://youtu.be/mjh5NIn1yHk) (watch the first five minutes only)
1. [Data generators and image augmentation](https://youtu.be/dSs3kjqvv_Q) 
1. [Image preprocessing](https://youtu.be/9_OFSSYcVWU)

**Notebook:** [Image preprocessing](./notebooks/Image_preprocessing.ipynb)

## 8. The convolution operation [The Most Important Topic] (Section 5.1.1) 

**Reading:** [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)

- [slides](https://docs.google.com/presentation/d/1uesCp63vUgzrgROJ7c3VfA5VyCR17W6czayhnWHj9AI/edit?usp=sharing) / [notebook](./notebooks/Detect_rectangles.ipynb)

1. Activations & loss functions (Sections 4.5.5, and Table 4.1) - [slides](https://docs.google.com/presentation/d/17Gx0Iaov1MuNRXWHAeIUQvz0wpdKWTaIAVQVyRrt-cs/edit?usp=sharing)
 
1. Classify MNIST digits using a CNN - [notebook](./notebooks/MNIST_v1.ipynb)

1. Evaluating models (Sections 4.2, 4.2.1, 4.2.2) - [slides](https://docs.google.com/presentation/d/1g8rzgspsYU90QtSV99hcA2_ojxJMwRdcw7KK55KNyMA/edit?usp=sharing)

1. Feature engineering (Section 4.3) - [slides](https://docs.google.com/presentation/d/14k2vUTlJThQ0u8RVc0C68_92K1Df5YW0v85C5w3nFe8/edit?usp=sharing) 

1. Overfitting, underfitting, & regularization (Sections 4.4, 4.4.1, 4.4.2, and 4.4.3) - [slides](https://docs.google.com/presentation/d/1RyqzBPX5_Cbs_sCsEJLmYWWK7hbScr2jJABV6blFxRU/edit?usp=sharing)

1. Workflow of machine learning (Sections 4.5, 4.5.5, 4.5.6, and 4.5.7) - [slides](https://docs.google.com/presentation/d/1jhp6E1B0M0Adf9jfv8OGZu2nv0p9Y1AmMy3KrwTWLFc/edit?usp=sharing) / [cheatsheet](https://docs.google.com/presentation/d/1mT4aHk0yx9dwxrfnr1WBSKYjEheYhf8R_x0NegrUsto/edit?usp=sharing)

1. GPUs for deep learning - [slides](https://docs.google.com/presentation/d/1Jg-BOZBDfhBht_3Sf49ja8QrWK_QuX7pr1CQkAf2mcI/edit?usp=sharing)

1. Classic CNN architectures (Sections 5.1.1, 5.1.2, and 7.1) - [slides](https://docs.google.com/presentation/d/1a5yeHRI_i0INatg9rLVpYuNTNvrxLCLxKH5_RISFwEY/edit?usp=sharing) / [notebooks](./notebooks/)

1. Transfer learning (Sections 5.3, 5.3.1, and 5.3.2) - [slides](https://docs.google.com/presentation/d/1OV2KDijNYVnwYUrpp0otFCGyt-mejSsvtArp3UyrMQM/edit?usp=sharing) / [notebook](./notebooks/Transfer_learning.ipynb)

1. Deep learning practices (Sections 5.4, 7.1.2, 7.1.3, 7.1.4, and 7.1.5) - [slides](https://docs.google.com/presentation/d/15qI0K9Sm4Ab1vp0x6fKyeCmweMZggTh237zfSxwj-B0/edit?usp=sharing)

1. Capsule networks - [slides](https://docs.google.com/presentation/d/1stzUpXvI889j1sziwbStpDItTfvBJ2ylrnMAJAxzLlo/edit?usp=sharing)

1. Explainable deep learning - [reading](https://neil.fraser.name/writing/tank/)

1. Limitations of DL & conclusions (Section 9.2) - [slides](https://docs.google.com/presentation/d/1uSlx4ojY3HXntjijSgA86e3DMyvoYzynjYOZSZ_ZFMg/edit?usp=sharing)
